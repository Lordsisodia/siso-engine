#!/usr/bin/env python3
"""
Unified Pipeline Orchestrator for BlackBox5

Combines the Feature Pipeline and Testing Pipeline into a single
automated system that:

1. Takes feature proposals
2. Reviews and simplifies them
3. Breaks them down into GSD tasks
4. Implements them
5. Tests them with auto-fix loop
6. Validates and completes

This creates a fully automated loop for implementing features
with minimal human intervention.
"""

import asyncio
import sys
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional

import yaml

from .feature_pipeline import FeaturePipeline, Feature, FeatureStatus, FeaturePriority
from .testing_pipeline import TestingPipeline, TestResult, TestStatus
from .Orchestrator import WorkflowResult


class PipelinePhase(str, Enum):
    """Phases of the unified pipeline"""
    FEATURE_REVIEW = "feature_review"
    FEATURE_BREAKDOWN = "feature_breakdown"
    IMPLEMENTATION = "implementation"
    TESTING = "testing"
    VALIDATION = "validation"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class PipelineRun:
    """
    Represents a single run of the unified pipeline.

    Attributes:
        run_id: Unique identifier
        feature: Feature being implemented
        phase: Current phase
        started_at: Start time
        completed_at: Completion time
        feature_review_result: Result from feature review
        workflow_result: Result from GSD implementation
        test_result: Result from testing
        validation_result: Result from validation
        errors: Any errors that occurred
    """
    run_id: str
    feature: Feature
    phase: PipelinePhase
    started_at: datetime
    completed_at: Optional[datetime]
    feature_review_result: Optional[Dict[str, Any]]
    workflow_result: Optional[WorkflowResult]
    test_result: Optional[TestResult]
    validation_result: Optional[Dict[str, Any]]
    errors: List[str]

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary"""
        return {
            'run_id': self.run_id,
            'feature_id': self.feature.feature_id,
            'feature_name': self.feature.name,
            'phase': self.phase.value,
            'started_at': self.started_at.isoformat(),
            'completed_at': self.completed_at.isoformat() if self.completed_at else None,
            'feature_review_result': self.feature_review_result,
            'workflow_summary': {
                'state': self.workflow_result.state.value if self.workflow_result else None,
                'steps_completed': self.workflow_result.steps_completed if self.workflow_result else 0,
                'steps_total': self.workflow_result.steps_total if self.workflow_result else 0
            } if self.workflow_result else None,
            'test_summary': {
                'status': self.test_result.status.value if self.test_result else None,
                'total': self.test_result.total_tests if self.test_result else 0,
                'passed': self.test_result.passed if self.test_result else 0,
                'failed': self.test_result.failed if self.test_result else 0
            } if self.test_result else None,
            'validation_result': self.validation_result,
            'errors': self.errors
        }


class UnifiedPipeline:
    """
    Unified pipeline orchestrator combining feature and testing pipelines.

    Provides a single interface for:
    - Proposing features
    - Reviewing and simplifying
    - Breaking down into GSD tasks
    - Implementing with GSD
    - Testing with auto-fix loop
    - Validating and completing
    """

    def __init__(
        self,
        blackbox_root: Path,
        enable_compression: bool = True,
        max_tokens_per_task: int = 8000,
        max_fix_iterations: int = 3
    ):
        self.blackbox_root = Path(blackbox_root)
        self.pipeline_dir = self.blackbox_root / "blackbox5" / "pipeline"
        self.runs_file = self.pipeline_dir / "pipeline_runs.yaml"

        # Ensure directories exist
        self.pipeline_dir.mkdir(parents=True, exist_ok=True)

        # Initialize sub-pipelines
        self.feature_pipeline = FeaturePipeline(
            blackbox_root=blackbox_root,
            enable_compression=enable_compression,
            max_tokens_per_task=max_tokens_per_task
        )
        self.testing_pipeline = TestingPipeline(blackbox_root=blackbox_root)

        # Configuration
        self.max_fix_iterations = max_fix_iterations

        # Load history
        self.runs: List[PipelineRun] = self._load_runs()

    def _load_runs(self) -> List[PipelineRun]:
        """Load pipeline runs from disk"""
        if not self.runs_file.exists():
            return []

        with open(self.runs_file, 'r') as f:
            data = yaml.safe_load(f)

        runs = []
        for item in data.get('runs', []):
            # Reconstruct feature
            from .feature_pipeline import Feature
            feature_data = item.get('feature')
            feature = Feature.from_dict(feature_data)

            runs.append(PipelineRun(
                run_id=item['run_id'],
                feature=feature,
                phase=PipelinePhase(item['phase']),
                started_at=datetime.fromisoformat(item['started_at']),
                completed_at=datetime.fromisoformat(item['completed_at']) if item.get('completed_at') else None,
                feature_review_result=item.get('feature_review_result'),
                workflow_result=None,  # WorkflowResult is not easily serializable
                test_result=None,  # TestResult is complex
                validation_result=item.get('validation_result'),
                errors=item.get('errors', [])
            ))

        return runs

    def _save_runs(self):
        """Save pipeline runs to disk"""
        data = {
            'updated_at': datetime.utcnow().isoformat(),
            'total_runs': len(self.runs),
            'runs': [r.to_dict() for r in self.runs]
        }

        with open(self.runs_file, 'w') as f:
            yaml.dump(data, f, default_flow_style=False, sort_keys=False)

    async def execute_full_pipeline(
        self,
        feature_name: str,
        feature_description: str,
        source_type: str = "internal",
        source_name: str = "manual",
        priority: FeaturePriority = FeaturePriority.MEDIUM
    ) -> PipelineRun:
        """
        Execute the full unified pipeline for a feature.

        This is the main entry point for automated feature implementation.

        Args:
            feature_name: Name of the feature
            feature_description: Detailed description
            source_type: Where the feature came from
            source_name: Name of the source
            priority: Priority level

        Returns:
            PipelineRun with complete execution results
        """
        import uuid

        run_id = str(uuid.uuid4())[:8]

        print(f"\n{'='*80}")
        print(f"üöÄ UNIFIED PIPELINE: {feature_name}")
        print(f"{'='*80}\n")

        # Create pipeline run
        run = PipelineRun(
            run_id=run_id,
            feature=None,  # Will be set after proposing
            phase=PipelinePhase.FEATURE_REVIEW,
            started_at=datetime.utcnow(),
            completed_at=None,
            feature_review_result=None,
            workflow_result=None,
            test_result=None,
            validation_result=None,
            errors=[]
        )

        try:
            # Phase 1: Propose Feature
            print(f"üìù Phase 1: Proposing Feature")
            print("-" * 40)

            feature = self.feature_pipeline.propose_feature(
                name=feature_name,
                description=feature_description,
                source_type=source_type,
                source_name=source_name,
                priority=priority
            )

            run.feature = feature
            print(f"‚úÖ Feature proposed: {feature.feature_id}")

            # Phase 2: Review Feature
            print(f"\nüîç Phase 2: Reviewing Feature")
            print("-" * 40)

            run.phase = PipelinePhase.FEATURE_REVIEW
            reviewed_feature = await self.feature_pipeline.review_feature(feature.feature_id)
            run.feature_review_result = reviewed_feature.metadata.get('review')

            if not run.feature_review_result.get('should_implement', False):
                print(f"‚ùå Feature review rejected implementation")
                print(f"   Reasoning: {run.feature_review_result.get('reasoning', 'Unknown')}")
                run.phase = PipelinePhase.FAILED
                run.errors.append("Feature review rejected implementation")
                run.completed_at = datetime.utcnow()
                self.runs.append(run)
                self._save_runs()
                return run

            print(f"‚úÖ Feature approved")
            print(f"   Simplified: {reviewed_feature.simplified_description}")
            print(f"   Acceptance Criteria: {len(reviewed_feature.acceptance_criteria)} items")

            # Phase 3: Implement Feature
            print(f"\nüî® Phase 3: Implementing Feature")
            print("-" * 40)

            run.phase = PipelinePhase.IMPLEMENTATION

            impl_result = await self.feature_pipeline.implement_feature(feature.feature_id)
            run.workflow_result = impl_result['workflow_result']

            if impl_result['workflow_result'].state.value != "completed":
                print(f"‚ùå Implementation failed")
                run.phase = PipelinePhase.FAILED
                run.errors.append(f"Implementation failed: {impl_result['workflow_result'].errors}")
                run.completed_at = datetime.utcnow()
                self.runs.append(run)
                self._save_runs()
                return run

            print(f"‚úÖ Implementation complete")
            print(f"   Steps: {impl_result['workflow_result'].steps_completed}/{impl_result['workflow_result'].steps_total}")
            print(f"   Waves: {impl_result['workflow_result'].waves_completed}")

            # Phase 4: Test Feature
            print(f"\nüß™ Phase 4: Testing Feature")
            print("-" * 40)

            run.phase = PipelinePhase.TESTING

            # Run test suite with auto-fix loop
            test_result = await self.testing_pipeline.run_test_suite(
                test_pattern=f"*{feature_name.lower()}*",
                max_iterations=self.max_fix_iterations
            )

            run.test_result = test_result

            if test_result.status != TestStatus.PASSED:
                print(f"‚ö†Ô∏è  Tests did not pass after {self.max_fix_iterations} iterations")
                print(f"   Passed: {test_result.passed}/{test_result.total_tests}")
                print(f"   Failed: {test_result.failed}")

                # Don't fail the pipeline, just warn
                run.errors.append(f"Tests had {test_result.failed} failures")
            else:
                print(f"‚úÖ All tests passed")
                print(f"   Total: {test_result.total_tests}")

            # Phase 5: Validate
            print(f"\n‚úÖ Phase 5: Validation")
            print("-" * 40)

            run.phase = PipelinePhase.VALIDATION

            # Check acceptance criteria
            reviewed_feature = self.feature_pipeline.get_feature(feature.feature_id)
            criteria_met = len(reviewed_feature.acceptance_criteria)
            criteria_total = len(reviewed_feature.acceptance_criteria)

            validation_result = {
                'acceptance_criteria_met': criteria_met,
                'acceptance_criteria_total': criteria_total,
                'tests_passed': test_result.status == TestStatus.PASSED,
                'implementation_complete': impl_result['workflow_result'].state.value == "completed"
            }

            run.validation_result = validation_result

            # Complete
            run.phase = PipelinePhase.COMPLETED
            run.completed_at = datetime.utcnow()

            # Mark feature as completed
            self.feature_pipeline.mark_completed(feature.feature_id)

            print(f"‚úÖ Feature validated and completed")
            print(f"   Acceptance criteria: {criteria_met}/{criteria_total}")
            print(f"   Tests passed: {validation_result['tests_passed']}")

        except Exception as e:
            print(f"\n‚ùå Pipeline failed with error: {str(e)}")
            run.phase = PipelinePhase.FAILED
            run.errors.append(str(e))
            run.completed_at = datetime.utcnow()

        # Save run
        self.runs.append(run)
        self._save_runs()

        print(f"\n{'='*80}")
        if run.phase == PipelinePhase.COMPLETED:
            print(f"‚úÖ PIPELINE COMPLETE")
        else:
            print(f"‚ö†Ô∏è  PIPELINE FINISHED: {run.phase.value}")
        print(f"{'='*80}\n")

        return run

    def get_run_history(self, limit: int = 10) -> List[PipelineRun]:
        """Get recent pipeline runs"""
        return self.runs[-limit:]

    def get_statistics(self) -> Dict[str, Any]:
        """Get pipeline statistics"""
        total_runs = len(self.runs)
        completed = len([r for r in self.runs if r.phase == PipelinePhase.COMPLETED])
        failed = len([r for r in self.runs if r.phase == PipelinePhase.FAILED])
        in_progress = len([r for r in self.runs if r.completed_at is None])

        return {
            'total_runs': total_runs,
            'completed': completed,
            'failed': failed,
            'in_progress': in_progress,
            'success_rate': completed / total_runs if total_runs > 0 else 0
        }


# CLI interface
def main():
    """CLI entry point for unified pipeline"""
    import argparse

    parser = argparse.ArgumentParser(
        description="BlackBox5 Unified Pipeline"
    )
    parser.add_argument(
        "command",
        choices=["run", "history", "stats"],
        help="Command to execute"
    )
    parser.add_argument("--name", type=str, help="Feature name")
    parser.add_argument("--description", type=str, help="Feature description")
    parser.add_argument("--source", type=str, help="Source type")
    parser.add_argument("--priority", type=str, help="Priority level")
    parser.add_argument("--iterations", type=int, default=3, help="Max fix iterations")
    parser.add_argument("--blackbox", type=Path, default=Path.cwd(), help="BlackBox5 root")

    args = parser.parse_args()

    async def run_command():
        pipeline = UnifiedPipeline(
            args.blackbox,
            max_fix_iterations=args.iterations
        )

        if args.command == "run":
            if not args.name or not args.description:
                parser.error("--name and --description required for run command")

            await pipeline.execute_full_pipeline(
                feature_name=args.name,
                feature_description=args.description,
                source_type=args.source or "internal",
                source_name="manual",
                priority=FeaturePriority(args.priority) if args.priority else FeaturePriority.MEDIUM
            )

        elif args.command == "history":
            history = pipeline.get_run_history()
            print(f"\nüìú Pipeline Run History ({len(history)})\n")

            for run in history:
                phase_emoji = "‚úÖ" if run.phase == PipelinePhase.COMPLETED else "‚ùå"
                print(f"   {phase_emoji} {run.started_at.strftime('%Y-%m-%d %H:%M')}")
                print(f"      Feature: {run.feature.name}")
                print(f"      Phase: {run.phase.value}")

                if run.completed_at:
                    duration = (run.completed_at - run.started_at).total_seconds()
                    print(f"      Duration: {duration:.1f}s")

        elif args.command == "stats":
            stats = pipeline.get_statistics()
            print(f"\nüìä Pipeline Statistics\n")
            print(f"   Total runs: {stats['total_runs']}")
            print(f"   Completed: {stats['completed']}")
            print(f"   Failed: {stats['failed']}")
            print(f"   In progress: {stats['in_progress']}")
            print(f"   Success rate: {stats['success_rate']:.1%}")

    asyncio.run(run_command())


if __name__ == "__main__":
    main()
